## Few interview questions on Jenkins, Groovy and YAML
```
https://chatgpt.com/share/67e3e8ae-c350-8011-9684-16cfd7d18d74
```

## How will you connect GitHub to Jenkins or Github to Jenkins connectivity
> 1. Install Necessary Plugins [Github plugins]
> 2. Add GitHub Credentials to Jenkins [SSH Privatekey or Username and password]
> - When Jenkins (via a job) connects to GitHub:
> - Jenkins needs an SSH key pair (private + public)
> - You add the public key to GitHub
> - Jenkins keeps the private key to authenticate

## Jenkins to VMs and PCF connectivity
>1. Using ssh keys[Private Key]
> - Use Jenkins credentials + ssh/scp in pipeline
> - Jenkins has an SSH private key stored in Manage Jenkins → Credentials
> - Your VMs have the matching public key in ~/.ssh/authorized_keys
>2. Using PAM ID

----------------------------------------------------------------------------------------

## Can you explain the CICD process in your current project ? or Can you talk about any CICD process that you have implemented ?
> In the current project we use the following tools orchestrated with Jenkins to achieve CICD. Maven, Sonar, fortify, Nexus and Kubernetes
>Coming to the implementation, the entire process takes place in 8 steps
>1. Code Commit: Developers commit code changes to a Git repository hosted on GitHub.
>2. Jenkins Build: Jenkins is triggered to build the code using Maven. Maven builds the code and runs unit tests.
>3. Code Analysis: Sonar is used to perform static code analysis to identify any code quality issues, security vulnerabilities, and bugs.
>4. Security Scan: AppScan is used to perform a security scan on the application to identify any security vulnerabilities.
>5. Deploy to Dev Environment: If the build and scans pass, Jenkins deploys the code to a development environment managed by Kubernetes.
>6. Continuous Deployment: ArgoCD is used to manage continuous deployment. ArgoCD watches the Git repository and automatically deploys new changes to the development environment as soon as they are committed.
>7. Promote to Production: When the code is ready for production.
>8. Monitoring: The application is monitored for performance and availability using Kubernetes tools and other monitoring tools.

## SonarQube: 
>SonarQube (formerly Sonar) is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells on 17 programming languages. SonarQube offers reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security recommendations.

## NexusIQ: 
>NexusIQ is used to scan the jars which we are using for the application to run. It will generate the report whether the jars are compatible or not.

## Fortify scan: 
>Fortify SCA is a static application security testing (SAST) offering used by development groups and security professionals to analyze the source code for security vulnerabilities. It reviews code and helps developers identify, prioritize, and resolve issues with less effort and in less time.

## Difference between Declarative and scripted pipeline
> Declarative Pipeline: A high-level syntax designed to simplify pipeline creation by providing a structured and user-friendly format.

> Scripted Pipeline: A lower-level pipeline syntax that provides maximum flexibility by using Groovy code.

## What are the different ways to trigger jenkins pipelines ?
> This can be done in multiple ways, To briefly explain about the different options,
>  **Build Triggers**: Jenkins can be configured to use the Git plugin, which allows you to specify a Git repository and branch to build. The plugin can be configured to automatically build when changes are pushed to the repository. 
>  - **Poll SCM**: Jenkins can periodically check the repository for changes and automatically build if changes are detected. This can be configured in the "Build Triggers" section of a job.                
>  - **Webhooks**: A webhook can be created in GitHub to notify Jenkins when changes are pushed to the repository. Jenkins can then automatically build the updated code. This can be set up in the "Build Triggers" section of a job and in the GitHub repository settings.

## How can we run jenkin pipelines in serially build job or post build actions?
#### Option 1: Run Jobs in Serial Using build job: (Pipeline Syntax)
>- This is used inside a pipeline script to trigger other jobs sequentially.
>- Example: Trigger jobs serially
```
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                echo "Building main project"
            }
        }
        stage('Trigger Next Job') {
            steps {
                build job: 'deploy-job'  // This job will run after 'Build'
            }
        }
    }
}
```
> You can trigger multiple jobs in order:
```
build job: 'job-1'
build job: 'job-2'
```
> These wait for the triggered job to complete before continuing.

#### Option 2: Serialize Jobs Using disableConcurrentBuilds()
> To ensure jobs don't overlap:
```
pipeline {
    agent any
    options {
        disableConcurrentBuilds()
    }
    stages {
        ...
    }
}
```
> This makes sure each job runs in serial, one at a time — even if triggered by another job or webhook.
#### Option 3: Use Post-Build Actions (Freestyle Jobs or Declarative)
> In freestyle jobs, you can:
>- Go to Post-build Actions ➜ Build Other Projects.
>- Add the next job(s) to trigger after the current one completes.
> In Jenkinsfile, do it like this:
```
post {
    success {
        build job: 'next-job-name'
    }
    failure {
        echo 'Build failed, not triggering next job'
    }
}
```
> This will trigger the next job only if the current pipeline passes.

## How to backup Jenkins ?
> Backing up Jenkins is a very easy process, there are multiple default and configured files and folders in Jenkins that you might want to backup.
>  - Configuration: The `~/.jenkins` folder. You can use a tool like rsync to backup the entire directory to another location.
>    - Plugins: Backup the plugins installed in Jenkins by copying the plugins directory located in JENKINS_HOME/plugins to another location.
>    - Jobs: Backup the Jenkins jobs by copying the jobs directory located in JENKINS_HOME/jobs to another location.
>    - User Content: If you have added any custom content, such as build artifacts, scripts, or job configurations, to the Jenkins environment, make sure to backup those as well.
>    - Database Backup: If you are using a database to store information such as build results, you will need to backup the database separately. This typically involves using a database backup tool, such as mysqldump for MySQL, to export the data to another location.
>One can schedule the backups to occur regularly, such as daily or weekly, to ensure that you always have a recent copy of your Jenkins environment available. You can use tools such as cron or Windows Task Scheduler to automate the backup process.

## How do you store/secure/handle secrets in Jenkins ?
> Again, there are multiple ways to achieve this, Let me give you a brief explanation of all the posible options.
>   - Credentials Plugin: Jenkins provides a credentials plugin that can be used to store secrets such as passwords, API keys, and certificates. The secrets are encrypted and stored securely within Jenkins, and can be easily retrieved in build scripts or used in other plugins.
>   - Environment Variables: Secrets can be stored as environment variables in Jenkins and referenced in build scripts. However, this method is less secure because environment variables are visible in the build logs. 
>   - Hashicorp Vault: Jenkins can be integrated with Hashicorp Vault, which is a secure secrets management tool. Vault can be used to store and manage sensitive information, and Jenkins can retrieve the secrets as needed for builds.
>   - Third-party Secret Management Tools: Jenkins can also be integrated with third-party secret management tools such as AWS Secrets Manager, Google Cloud Key Management Service, and Azure Key Vault.

## What is latest version of Jenkins or which version of Jenkins are you using ?
> This is a very simple question interviewers ask to understand if you are actually using Jenkins day-to-day, so always be prepared for this.
> Latest version: 2.479.2

## What is shared modules in Jenkins ?
> Shared modules in Jenkins refer to a collection of reusable code and resources that can be shared across multiple Jenkins jobs. This allows for easier maintenance, reduced duplication, and improved consistency across multiple build processes. For example, shared modules can be used in cases like:
>- Libraries: Custom Java libraries, shell scripts, and other resources that can be reused across multiple jobs.
>- Jenkinsfile: A shared Jenkinsfile can be used to define the build process for multiple jobs, reducing duplication and making it easier to manage the build process for multiple projects.     
>- Plugins: Common plugins can be installed once as a shared module and reused across multiple jobs, reducing the overhead of managing plugins on individual jobs.       
>- Global Variables: Shared global variables can be defined and used across multiple jobs, making it easier to manage common build parameters such as version numbers, artifact repositories, and environment variables.

## can you use Jenkins to build applications with multiple programming languages using different agents in different stages ?
> - Yes, Jenkins can be used to build applications with multiple programming languages by using different build agents in different stages of the build process.
> - Jenkins supports multiple build agents, which can be used to run build jobs on different platforms and with different configurations. By using different agents in different stages of the build process, you can build applications with multiple programming languages and ensure that the appropriate tools and libraries are available for each language.
> - For example, you can use one agent for compiling Java code and another agent for building a Node.js application. The agents can be configured to use different operating systems, different versions of programming languages, and different libraries and tools.
> - Jenkins also provides a wide range of plugins that can be used to support multiple programming languages and build tools, making it easy to integrate different parts of the build process and manage the dependencies required for each stage.
> - Overall, Jenkins is a flexible and powerful tool that can be used to build applications with multiple programming languages and support different stages of the build process.

## How to setup auto-scaling group for Jenkins in AWS ?
> Here is a high-level overview of how to set up an autoscaling group for Jenkins in Amazon Web Services (AWS):
>    - Launch EC2 instances: Create an Amazon Elastic Compute Cloud (EC2) instance with the desired configuration and install Jenkins on it. This instance will be used as the base image for the autoscaling group.
>    - Create Launch Configuration: Create a launch configuration in AWS Auto Scaling that specifies the EC2 instance type, the base image (created in step 1), and any additional configuration settings such as storage, security groups, and key pairs.   
>    - Create Autoscaling Group: Create an autoscaling group in AWS Auto Scaling and specify the launch configuration created in step 2. Also, specify the desired number of instances, the minimum number of instances, and the maximum number of instances for the autoscaling group.
>    - Configure Scaling Policy: Configure a scaling policy for the autoscaling group to determine when new instances should be added or removed from the group. This can be based on the average CPU utilization of the instances or other performance metrics.
>    - Load Balancer: Create a load balancer in Amazon Elastic Load Balancer (ELB) and configure it to forward traffic to the autoscaling group.
>    - Connect to Jenkins: Connect to the Jenkins instance using the load balancer endpoint or the public IP address of one of the instances in the autoscaling group.    
>    - Monitoring: Monitor the instances in the autoscaling group using Amazon CloudWatch to ensure that they are healthy and that the autoscaling policy is functioning as expected.
> By using an autoscaling group for Jenkins, you can ensure that you have the appropriate number of instances available to handle the load on your build processes, and that new instances can be added or removed automatically as needed. This helps to ensure the reliability and scalability of your Jenkins environment.

## How to add a new worker node in Jenkins ?
> Log into the Jenkins master and navigate to Manage Jenkins > Manage Nodes > New Node. Enter a name for the new node and select Permanent Agent. Configure SSH and click on Launch.

## How to add a new plugin in Jenkins ?
> Using the CLI, java -jar jenkins-cli.jar install-plugin <PLUGIN_NAME>
>
> Using the UI,
> - Click on the "Manage Jenkins" link in the left-side menu.
> - Click on the "Manage Plugins" link.

## What is JNLP and why is it used in Jenkins ?
> In Jenkins, JNLP[Java Network Launch Protocol] is used to allow agents (also known as "slave nodes") to be launched and managed remotely by the Jenkins master instance. This allows Jenkins to distribute build tasks to multiple agents, providing scalability and improving performance.
> When a Jenkins agent is launched using JNLP, it connects to the Jenkins master and receives build tasks, which it then executes. The results of the build are then sent back to the master and displayed in the Jenkins user interface.

## What are some of the common plugins that you use in Jenkins ?
> Be prepared for answer, you need to have atleast 3-4 on top of your head, so that interview feels you use jenkins on a day-to-day basis.
> - Maven: Maven Integration Plugin, Pipeline Maven Integration, ArtifactDeployer Plugin
> - PCF: Cloud Foundary CLI, Blue Green Deployment, Pipeline Utility steps
> - AWS: Amazon EC2, AWS Elastic Beanstalk, S3 Publisher, AWS CodeDeploy.

## Advantages of Jenkins over GitHub Actions
>Integration: Jenkins can integrate with a wide range of tools and services, but GitHub Actions is tightly integrated with the GitHub platform, making it easier to automate tasks related to your GitHub workflow.
>In conclusion, Jenkins is better suited for complex and large-scale automation tasks, while GitHub Actions is a more cost-effective and user-friendly solution for simple to moderate automation needs.

## For suppose we have jenkins and sonarqube in single server how they interact with each other inside the server
>When both Jenkins and SonarQube are installed on the same server, they can still interact with each other in the same way as they would if they were on different servers. The main difference is that since both tools reside on the same server, they will communicate over localhost (i.e., 127.0.0.1 or localhost), which simplifies the connection process because there's no need for cross-server communication or firewall configurations.

## Pipeline Optimization Techniques:
> Multibranch Pipeline, Master and slave technique and assigning specific agent in the script for that application.

## What is the difference between variables and parameters in jenkins
### Parameters
>- Defined by the user before a build starts.
>- Appear in the Jenkins UI when you manually trigger a build.
>- Allow users to input values to customize a job run.
>- Declared using the parameters block in a Jenkinsfile.
```
pipeline {
    parameters {
        string(name: 'USERNAME', defaultValue: 'guest', description: 'Name of the user')
    }
    stages {
        stage('Greet') {
            steps {
                echo "Hello, ${params.USERNAME}!"
            }
        }
    }
}
```
### Variables
>- Used internally within the pipeline script.
>- Can be defined in steps, stages, or globally in the pipeline.
>- Often used to store temporary values or results from commands.
```
pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                script {
                    def greeting = "Hello, World!"
                    echo greeting
                }
            }
        }
    }
}
```

## What is difference between agents and labels in jenkins
### Agent
>- The agent defines where the pipeline or a particular stage should run.
>- It can be the master (now called controller) or any other node (agent/slave).
>- Specified in the pipeline or stage block.
>- You can use agent any, agent none, or specify a label.
```
pipeline {
    agent any  // Run on any available agent
}
```
### Label
>- A label is a tag or name assigned to Jenkins nodes (agents).
>- Used to group nodes with similar capabilities—e.g., linux, windows, docker, gpu, etc.
>- Helps Jenkins decide which agent to use when you request a certain label.
#### Labels are defined in Jenkins node configuration:
> Manage Jenkins ➜ Manage Nodes ➜ [Node Name] ➜ Configure ➜ Labels
```
pipeline {
    agent {
        label 'linux'  // Run on an agent with label 'linux'
    }
}
```

## Suppose we have git push stage and stage is getting failed and credentials are working fine. What is the probability and how can we troubleshoot
### 1. Double-Check the Error Message
> Even if the credentials are working, the actual error message from the failed stage is gold. It might say:
>- Permission denied
>- remote: error: refusing to update checked out branch
>- non-fast-forward
>- could not read from remote repository
>- not a git repository

### 2. Common Causes When Credentials Work
#### a. You're Trying to Push to a Protected Branch
>- Some repos (e.g., on GitHub, GitLab) block direct pushes to main or master.
>- Even valid credentials won’t work if branch protection is enabled.
##### Fix:
>- Check repo settings (on GitHub/GitLab/etc).
>- Use a pull request workflow instead of direct push.

#### b. You're on a Detached HEAD
>- Jenkins clones repos in detached HEAD state by default (not on a named branch).
>- If you try to push, Git doesn't know which branch to push to.
##### Fix:
```
sh 'git checkout -b your-branch-name' // before push
```
#### c. Remote is Not Set or Misconfigured
> Jenkins does a shallow clone sometimes.
> Remote name (origin) might be missing or pointing wrong.
##### Fix:
```
git remote -v  # check remotes
git remote add origin <repo-url>  # if missing
```
#### d. Missing Git User Config in Jenkins Agent
> Git push may require user.name and user.email.
##### Fix:
```
git config user.name "Jenkins"
git config user.email "jenkins@example.com"
```
#### e. You're Not Pushing the Right Branch
> If Jenkins is on a branch Jenkins created or checked out, your push might look like:
```
git push origin HEAD:refs/heads/main
```
##### Use an explicit push like:
```
git push origin your-branch
```

## Parallel staging:
```
pipeline {
    agent any

stages {
        stage('Parallel Stages') {
            parallel {
                stage('Stage 1') {
                    steps {
                        echo "Running Stage 1"
                        sleep 10
                    }
                }
                stage('Stage 2') {
                    steps {
                        echo "Running Stage 2"
                        sleep 10
                    }
                }
            }
        }
    }
}
```

## What is NEXUS artifact repository
> Nexus Repository (often just called Nexus) is a universal artifact repository manager developed by Sonatype. It's used to store, manage, and serve build artifacts (like JARs, WARs, Docker images, NPM packages, etc.) for software projects.
> 
> Think of it like a warehouse for all your project's dependencies and build outputs — used heavily in CI/CD pipelines.

## How do you upload nexus artifact and how did you configured it in pipeline?
### 1. Basic Setup: What You Need
> To upload to Nexus, you need:
>- Nexus URL (like https://nexus.example.com)
>- Repository name (like maven-releases, maven-snapshots, etc.)
>- Credentials (username/password or token)
>- Group ID, Artifact ID, Version (standard Maven stuff)
>- Artifact file (.jar, .zip, etc.)
### 2. Configure Nexus Credentials in Jenkins
>- Go to Jenkins Dashboard ➜ Manage Jenkins ➜ Credentials.
>- Add new Username with Password or Secret Text.
>- Note the Credential ID (you’ll use this in the pipeline).
### 3. Pipeline Example: Upload JAR to Nexus
> Using curl (no plugins needed):
```
pipeline {
    agent any

    environment {
        NEXUS_URL = 'https://nexus.example.com/repository/maven-releases/'
        CRED_ID = 'nexus-credentials-id'  // Jenkins credentials ID
    }

    stages {
        stage('Build') {
            steps {
                sh 'mvn clean package'  // or any build step
            }
        }

        stage('Upload to Nexus') {
            steps {
                withCredentials([usernamePassword(credentialsId: env.CRED_ID, usernameVariable: 'USERNAME', passwordVariable: 'PASSWORD')]) {
                    sh """
                        curl -v --user $USERNAME:$PASSWORD --upload-file target/myapp-1.0.0.jar \\
                        ${NEXUS_URL}com/example/myapp/1.0.0/myapp-1.0.0.jar
                    """
                }
            }
        }
    }
}
```
> The upload URL format is usually:
```
<NEXUS_URL>/<groupId (with slashes)>/<artifactId>/<version>/<artifact>.jar
```
## How do you authenticate jenkins and nexus repository
> There are 3 main ways to authenticate:
#### a. Username & Password
>- Most common, especially for legacy setups.
>- You create a user in Nexus, then store that in Jenkins Credentials.
#### b. Username & API Token
>- Safer than passwords. Nexus (especially Nexus 3) allows generating tokens per user.
>- Acts like a password but can be revoked.

#### c. Deploy Key / Secret Token (for hosted repos)
>- You can create repo-specific deploy tokens in Nexus (depends on Nexus version and repo type).
>- Often used for automation-only access.

## What are the jenkins pipelines used in current project
> We're using a Free style Jobs and multibranch declarative pipeline, triggered by GitHub webhooks. The pipeline runs through checkout, build, unit testing, artifact upload to Nexus, and deployment to Dev and QA environments. For releases, we have a manual approval gate before deploying to staging and prod.

## Loops in Groovy

### each loop:
```
def numbers = [1, 2, 3, 4, 5]
numbers.each { num ->
    println "Number: $num"
}
```
### for loop and break
```
for (i in 1..5) {
    if (i == 3) break
    println "i: $i"
}
```

## Post Block:
```
post {
        always {
            echo 'This always runs, regardless of success or failure.'
        }
        success {
            echo 'Pipeline succeeded!'
        }
        failure {
            echo 'Pipeline failed!'
        }
        unstable {
            echo 'Pipeline is unstable!'
        }
        changed {
            echo 'Pipeline result changed compared to the previous run!'
        }
    }
```

## What are the checkpoints used in Sonarqube
#### 1. Project Configuration Check
>- What it does: Ensures the project is set up correctly with the necessary metadata (project key, name, language, etc.).
>- When it happens: Before analysis begins.
>- Failure example: Missing sonar.projectKey or misconfigured server.

#### 2. Source Code Collection
>- What it does: Gathers the source files, test files, and coverage reports (if available).
>- Includes:
>- Language detection
>- File exclusions/inclusions
>- Failure example: If no valid source code is detected, analysis may be skipped.

#### 3. Static Code Analysis
>- What it does: Runs a set of language-specific rules to find:
>- Bugs
>- Code Smells
>- Vulnerabilities
>- Security Hotspots
> Includes:
>- Syntax parsing
>- Semantic analysis
>- Rule engine evaluation
#### 4. Code Coverage & Test Results (Optional but Recommended)
>- What it does: Integrates external coverage tools (like Jacoco, Istanbul, etc.) and shows:
>- Test coverage %
>- Line/branch coverage
>- Checkpoint: Ensures test reports are found and correctly parsed.
#### 5. Issue Detection & Indexing
>- What it does: Collects issues (bugs, smells, vulnerabilities) and tags them with severity, location, and remediation effort.
>- Checkpoint: Issues are classified and stored for reporting and historical tracking.

#### 6. Quality Gate Evaluation (Critical Checkpoint)
>- What it does: Compares analysis results against the quality gate criteria, such as:
>- No new bugs
>- 80%+ coverage on new code
>- Maintainability rating of A on new code
>- Outcome: Passed or Failed
>- This is the most important checkpoint in CI/CD pipelines.
#### 7. Report Upload to SonarQube Server
>- What it does: Sends the full analysis report to the SonarQube server.
>- Failure example: Network error, authentication issue, or misconfigured SonarQube token.

#### 8. Dashboard Update
>- What it does: Displays the new analysis in the SonarQube UI with updated metrics, issues, trends, etc.

## How will you give threshold value to find the bugs and repatative code in Sonarqube
> Recommended Way: Use SonarQube Quality Gate
>- Go to SonarQube UI > Quality Gates
>- Set rules like:
>- Bugs on New Code <= 0
>- Code Duplication on New Code < 3%
>- Maintainability Rating = A

## What will happen when we have more bugs in Source code in the sonar report
> In Jenkins (or any CI/CD tool):
>- If you're using the waitForQualityGate step in a pipeline:
```
waitForQualityGate abortPipeline: true
```
> Then:
>- If Quality Gate passes → pipeline continues
>- If Quality Gate fails (e.g., too many bugs) → pipeline stops immediately
> This prevents deployment of poor-quality code.
```
stage('SonarQube Analysis') {
    steps {
        withSonarQubeEnv('SonarQubeServer') {
            sh 'mvn sonar:sonar'
        }
    }
}

stage('Quality Gate') {
    steps {
        timeout(time: 5, unit: 'MINUTES') {
            waitForQualityGate abortPipeline: true  // Fails if too many bugs
        }
    }
}
```