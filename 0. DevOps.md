

DevOps:

DevOps is the combination of development and operations to increase the efficiency, speed and security of software development and delivery compared to traditional processes.

It means addressing the traditional issues and implementing complete automation at every stage of SDLC through automation tools.



Software Development Life Cycle (SDLC):

SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software.






Cloud Computing: 

Cloud computing is the delivery of computing services over the internet. Computing services include common IT infra such as virtual machines, storage, databases and networking. It means just like power and software you need to run the software instead of physically without you.

Infrastructure as a Service (IaaS):

It places the most responsibility on the consumer, with the cloud provider being responsible for the basics of physical security, power and connectivity.

Platform as a Service (PaaS):

It is a middle ground between IaaS and SaaS and evenly distributes responsibility between consumer and cloud provider.

Software as a Service (SaaS):

It places most of the responsibility with the cloud provider.





Monolithic Architecture: A monolithic application is built as a single, unified unit where all functionalities are tightly coupled and run as one single large application.

Microservice Architecture: In microservices architecture the application is divided into small, independent services, each responsible for specific functionality.

CI:

Continuous integration is a DevOps software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. Continuous integration most often refers to the build or integration stage of the software release process

CD:

Continuous deployment is a strategy for software releases wherein any code commit that passes the automated testing phase is automatically released into the production environment, making changes that are visible to the software's users.

CI/CD:

A Continuous integration and Continuous Deployment is a series of steps that must be performed in order to deliver a new version of software. CI/CD are a practice focused on improved software delivery throughout the Software Development Life Cycle via Automation.

Whenever, developers write the code of all the developers at that point of time and we build, test and deploy to the client. This process is known as CI/CD. Jenkins will help us to achieve this.

How is DevOps different from agile methodology?

DevOps is a culture that allows the development and the operations team to work together. This results in continuous development, testing, integration, deployment, and monitoring of the software throughout the lifecycle. DevOps addresses gaps and conflicts between the Developers and IT Operations.

Agile is a software development methodology that focuses on iterative, incremental, small, and rapid releases of software, along with customer feedback. It addresses gaps and conflicts between the customer and developers.

Tools used in DevOps:

Git:

Git is the most commonly used version control system. Git tracks the changes you make to files, so you have a record of what has been done, and you can revert to specific versions should you ever need to. Git also makes collaboration easier, allowing changes by multiple people to all be merged into one source.

There are total of 4 stages in Git

1. Workspace:-
It is the place where we can create files physically and modify them. Being a Git user, we work in this work space.

2. Local repository:-
It is the place where Git stores all commit locally. It is a hidden directory so that no one can delete it accidentally. Every commit will have a unique commit ID.

3. Staging area/Indexing area:-
In this area, Git takes a snapshot for every version. It is a buffer zone between workspace and local repository. We can’t see this region because it is virtual.

4. Central repository:-
It is the place where Git stores all commit centrally. It belongs to everyone who is working on your project. GitHub is one of the central repositories. Used for storing the code and sharing the code to others in the team.

Git Reset command is used to remove changes from the staging area. This is bringing back files from the staging area to the work directory.
Git Revert command is used to remove changes from all 3 stages (work directory, staging area and local repository). We use this command after committing.
Git Clone command is used for the first time if you want a whole central repository in your local server.
Git pull is to get only new changes from the central repository.
Git fetch means, only bringing changes from central repo to local repo.
Git merge, one new merge commit will be generated which has the history of both development branches.
Git rebase, commits in the new branch will be applied on top of the base branch tip.
Git Hooks, We often call this as web hooks as well. In simple terms, hooks are nothing but scripts to put some restrictions.
Pre-commit hooks:- Sometimes you would want every member in your team to follow a certain pattern while giving a commit message. Then only it should allow them to commit. These types of restrictions we call pre-commit hooks.
Post-commit hooks:- Sometimes, being a manager you would want an email notification regarding every commit occurring in a central repository. This kind of thing we call post-commit hooks.
SVN:- It is a centralized version control system (CVCS) where backup copy will be placed in only one central repository. There is no branching strategy in SVN. You can’t create branches. So no parallel development.
Git:- It is a Distributed version control system where back up copy is available in everyone’s machine’s local repository as well as a central repository. We can create any branch as we want. So we can go in parallel development simultaneously.
Git Commit, Every time we commit, while committing, we have to give a commit message just to identify each commit. We can’t remember to commit numbers because they contain 40 long alphanumeric characters. So, to remember commits easily, we give a commit message. The format of commit message differs from company to company and individual to individual.



Jenkins:

Jenkins is a tool that is used for automation, and it is an open-source server that allows all the developers to build, test and deploy software. By using Jenkins we can make a continuous integration of projects(jobs) or end-to-end point automation.

Integrate: Combine all code written by developers till some point of time.
Build: Compile the code and make a small executable package.
Test: Test in all environments whether the application is working properly or not.
Archived: Stored in an artifactory so that in future we may use/deliver again.
Deliver: Handing the product to Client.
Deploy: Installing product in client’s machines.
Jenkins Workflow: We attach Git, Maven, Selenium & Artifactory plug-ins to Jenkins. Once Developers put the code in Git, Jenkins pulls that code and sends it to Maven for build. Once build is done, Jenkins pulls that built code and sends it to selenium for testing. Once testing is done, then Jenkins will pull that code and send it to Artifactory as per requirement and finally we can deliver the end product to a client we call Continuous Delivery. We can also deploy Jenkins into the client's machine directly as per the requirement. This is the Jenkins workflow.
Ways to achieve CI: 
Manually: – Manually write code, then do build manually and then test manually by writing test cases and deploy manually into clients machine.
Scripts: – Can do the above process by writing scripts so that these scripts do CI&CD automatically. But here complexity is, writing a script is not so easy.
Tool: – Using tools like Jenkins is very handy. Everything is preconfigured in these types of tools. So less manual intervention. This is the most preferred way.
Benefits of CI: 
Detects bugs as soon as possible, so that bugs will be rectified fast and development happens fast.
Complete automation. No manual intervention.
We can intervene manually whenever we want. i.e. We can stop any stage at any point of time so have better control.
Can establish a complete and continuous workflow.
Types of configurations in Jenkins:
Global: – Here, whatever configuration changes we do, applicable to the whole Jenkins including jobs as well as nodes. This configuration has high priority. 
Job: – These configurations are applicable to only Jobs. Jobs are also what we call projects or items in Jenkins.
Node: – These configurations are applicable to only nodes. Also we call Slaves. These are kind of helpers to Jenkins master to distribute the excessive load.


Build:

Gradle: ./gradlew clean build, chmod 775 gradlew && ./gradlew build
Maven: mvn -s clean build, ls -al ${WORKSPACE}/target

SonarQube:

SonarQube (formerly Sonar) is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells on 17 programming languages. SonarQube offers reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security recommendations.

NexusIQ:

NexusIQ is used to scan the jars which we are using for the application to run. It will generate the report whether the jars are compatible or not.

Fortify scan:

Fortify SCA is a static application security testing (SAST) offering used by development groups and security professionals to analyze the source code for security vulnerabilities. It reviews code and helps developers identify, prioritize, and resolve issues with less effort and in less time.

Nexus:

Nexus repository is used to store the artifacts. Artifacts are of two types: Snapshot version and Release version.

What is Blue-Green Deployment:

Blue and Green mean two identical Production environments. Which means, we will have two identical production environments and among those, one will serve the end-user at a time.
Say, we have Production Environment-1 called as Blue and Production Environment-2 called as Green. And All the End User traffic will be routed to Green Environment which is in Version-1.
Then, We Upgrade the Blue environment to Version-2. and perform all sanity and Production Testing to check the installation is correct or not
After that, once the Production Testing is passed in the Blue environment, we slowly route all the users to the Blue environment which is installed with Version 2.
Finally, we will install the same packages to Green Environment and again we route End Users back to Green Environment.
Weblogic: 
WebLogic servers are called middleware or application servers. They are called so as they act as a middle layer between the front end user interface and the backend database.
WebLogic servers host the applications or the business logic required to perform any action. For example, when you click a button on a webpage, the request goes from your browser to the WebLogic server, the application decides what action is to be taken for the request, queries the database and gives the response on your screen.
Steps of creating Weblogic Domain:
Admin server:
Initially weblogic software needs to be installed on a Linux server.
We have to create two files: create_domain.sh file and domain_config.py file.
Create_domain.sh file will install the java patch and it will call the .py file.
Domain_config.py file will call the weblogic wls.jar file
We have to run the create_domain.sh file, it will create the weblogic or weblogic12C folder in the specified path which is mentioned in the .py file and also it will create the domain folder.
Inside the domain folder, there will be a bin folder. Inside the bin has a set of files.
Next, we have to update the startWeblogic.sh file as per the requirements and we have to run the startWeblogic.sh file. During runtime it will ask the username and password for the first time those details will be mentioned in the .py file, we have to enter same details.
In the console, it will show as running. We have to make it run in the background by using the command ctlz + bg.
Then the admin server will be accessible through console and we can also check using ps -ef | grep java command, it will display the PID details.
We can access the console using the “hostname:portno/console”. We have to enter username and password.
Node Manager:
We have to update the nodemanager.properties and nodemanager.domain as per our requirements.
In the console we have to create the machines and nodemanagers as per the specifications.
Now run the startNodeManager.sh file. It will start the nodemanager.
Managed server:
We have to configure the managed servers in the console.
Then we can start the managed servers.
We have to configure the data source by using the JDBC url.
PCF: 
Pivotal Cloud Foundry, also known as PCF, is a distribution of the open-source Cloud Foundry platform that includes additional features and services that expand the capabilities of Cloud Foundry and make it easier to use.
Groovy Script:
Groovy Script is a java based object oriented programming and scripting language which runs on Java Virtual Machine. Groovy has very similar syntax with Java language.
Mandatory Parts:
Agent: Slave on which jenkins pipeline is executed.
Stage: A stage is a block in which tasks are performed through a jenkins pipeline.
Stages: Block that includes multiple stage in a pipeline.
Steps: All the tasks performed in any stage.
YAML:
YAML means yet another markup language. It is a data serialization language that works well with the modern programming language, and it is human-friendly.
It is used to define the data structures that are very easy to understand. These data structures are very easy to manage and maintain by the users.
YAML uses space to define something. If we use a single space or double space, it has different meanings in YAML. Spaces change the meaning of data structure.
YAML is case sensitive.
About CONDOR Project:
CONDOR: Capitalizing On Digital and Optimizing Our Resource
Initially start working on an application, we have to go through the 3 documents namely AIF, TMD and ADF.
App Intake Form describes the on-prem application details like what tools they have implemented, which data center it is located in on-perm, how components the application has etc.
Technical Migration Document describes the architecture of the application and some details about the migration plan.
Application Development Report or Assessment Form describes what changes need to be done to application before deploying to the new Dataceters.
After analyzing all the 3 documents, we have to start working on the application.
First we need to check whether it is a PCF deployment application (PaaS) or Linux Server application (IaaS).
We have  to check whether the application is on-boarded to the CI tools. If it is not onboarding we have to raise a request in the self service portal.
During deployment, we have to follow the sequence of components to deploy. If the application has Discovery service or Eureka Service, Config service and some application components. First we have to deploy Eureka service then next config service. After successfully deploying these two components, we should deploy application service components.
We have to support both Development, Release, Staging and Production Environment.
Eureka Service: Eureka Server is an application that holds the information about all client-service applications. Every Micro service will register into the Eureka server and Eureka server knows all the client applications running on each port and IP address. Eureka Server is also known as Discovery Server.
Config Service: Microservice configuration management is defined as the process of tracking changes to microservices and their consuming applications over time. In microservices, you have multiple applications and multiple instances to configure and manage.


PaaS application:
In ADF, we need to check how many components need to be deployed.
In ADF we need to check which level (L1, L2, L3….) we need to deploy in COLO data centers.
We need to check the PCF space, whether space is created or not for that particular EAI.
Create/Modify the pipeline as per the above analysis for both CI and CD as well.
If any new CI tool is mentioned to implement in ADF, we have to add that stage in the jenkins file.
Create a separate Jenkins job for COLO pipeline and make the configurations and trigger the build.
After deploying the application into PCF, verify whether the application health check is up and running.
For GSLB configuration, we need to add a routes section in the manifest file. After configuring the GSLB, we need to verify if it is working or not.
IaaS based deployment:
 First, we need to verify the AIF, ADF and TMD of the particular EAI.
In ADF, we need to check how many components need to be deployed.
In ADF we need to check which level (L1, L2, L3….) we need to deploy in COLO data centers.
We need to check whether the VM’s are created or not of particular EAI in CloudOps.
After creating the VM’s, we need to do the SSH onboarding Or we can create manually using Linux Commands.
If the application is weblogic, before deploying the application we need to configure the weblogic by creating domain and managed servers in the console.
Create/Modify the pipeline as per the above analysis for both CI and CD as well.
Create a separate Jenkins job for COLO pipeline and make the configurations and trigger the build.
After deploying the application into VMs, verify whether the application health check is up and running by using process command or by accessing the end point url of the application in SAG server.
Verify whether the load balancer is working or not.


CookBooks and New Implementations:
Jenkins Vault Configuration for IaaS application:
One of the IaaS applications, we have 3 components: Discovery, Config and Application service.
In the Config Repo we have application property files.
In property files we have passwords which are in the form plain text. 
As per the one of the min, we should not have such type of plain text of passwords in any files.
First we have to move those credentials into the Jenkins vault. We have copied those vault IDs and we have to define them in the Scripted Jenkins file.
Before running the start script, we need to export those variables which carry the value of credentials. 
In the Deployment, we have to export every variable.
Second, Before deploying the config server we have to update the application.yml.
Whatever the properties we have in the application.properties file with credentials, we should copy into the application.yml which is the present resource folder as per the YAML format. We should copy those properties under the override tag.
Now we have to replace plain text username and password with variables which we have exported in the jenkins file.
Then we can proceed for deployment.
Docker:

Docker is the containerization platform that is used to package your application and all its dependencies together in the form of containers to make sure that your application works seamlessly in any environment which can be developed or tested or in production. Docker is a tool designed to make it easier to create, deploy, and run applications by using containers.

Containers:

Containers are isolated from one another and bundle their own software, libraries, and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and therefore use fewer resources than a virtual machine.



Kubernetes:

Kubernetes is an open-source Container Management tool which automates container deployment, container scaling, and descaling and container load balancing (also called as container orchestration tool).



