## Few questions on kubernetes
```
https://chatgpt.com/share/67ebe53d-ddf4-8011-83d8-04e31d3f9ee4
```
```
https://chatgpt.com/share/67ece876-dca0-8011-9ff0-cd009fa41fe4
```
## Helm charts
```
https://chatgpt.com/share/67f38bae-a5e0-8011-9d96-31cafbc897f7
```

## What is kubernetes?
> - Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.
> - (often abbreviated as K8s - The abbreviation K8s comes from: Kubernetes has 10 characters between the first letter K and the last letter s.)

> **Node**: In Kubernetes, a node is a worker machine in the cluster where workloads (i.e., Pods) are deployed and executed.

> **Container**: It is a lightweight, portable, and self-sufficient unit that packages an application and its dependencies, allowing it to run consistently across different environments

> **POD**: A Pod is the smallest and most basic deployable unit in Kubernetes. It represents a single instance of a running process in the cluster and serves as a wrapper for one or more containers.

> **Cluster**: A Kubernetes cluster is a collection of machines (nodes) that work together to run, manage, and scale containerized applications.

## Advantages of Kubernetes
> Cluster in nature or behaviour, Auto healing, Auto Scaling, provides multiple enterprise level support[Load balancer, secure networking]

## Architecture of kubernetes?
> Control Plane [Master]- API Sever, etcd, scheduler, controller manager, cloud controller manager
>
>Data Plane [Worker Node]- Kubelet, kube-proxy, container-runtime
>
>1. We user want to create a POD request goes through **Control plane**
>2. In Control plan, **API Server** validates the Pod manifest to ensure it follows Kubernetes schema and syntax. It writes the desired state into **etcd**, the distributed key-value store, which acts as the cluster's source of truth.
>3. The **Kubernetes Scheduler** determines the best node for the Pod. Once the Scheduler selects a node, it updates the Pod object in **etcd** with the node assignment.
>4. The **Kubelet** detects the new Pod assignment. It communicates with the 'container runtime' (e.g., containerd, CRI-O) to perform the following:
>    - Pull the container image from the container registry (e.g., Docker Hub or a private registry) if it is not already cached on the node.
>    - Create and start the container(s) as specified in the Pod manifest.
>    - Configure networking and storage for the Pod.
>5. **Kube-proxy** will assign the ip-address, networking.
>6. **Persistent Volume Attachments (if any):** If the Pod specifies volumes (e.g., for persistent storage), Kubernetes ensures the volumes are provisioned, attached, and mounted to the container.
>7. **Readiness and Liveness Probes (if configured):** Kubernetes monitors the health of the container(s) in the Pod using readiness and liveness probes.
>    - If a container fails a liveness probe, Kubernetes restarts it.
>    - If a container fails a readiness probe, it is removed from the Service endpoints, ensuring it doesn't receive traffic.

## Role of Components
>**Kubelet** component will responsible for maintaining or managing a POD. It always ensures POD is always running

>**Container runtime** is responsible for managing the lifecycle of containers on each node in the cluster. The container runtime ensures that containers are created, started, stopped, and deleted as specified by Kubernetes.

>**Kube-proxy** basically acts as networking and allocate ip address, default load balancing capabilities. Generally it uses IP tables with is present in Linux machines.

>**API Server** is a component basically exposes to the world. It will takes all the inputs from users and validate the syntax.

>**Scheduler** is basically responsible for schudeling POD or resorces on kubernetes. It was decided by API server and according to that scheduler acts on it.

>**etcd** is a key value store which stores all the data or configurations of kubernetes. Basically it is a back-up of kubernetes.

>**Controller manager** has some controller which ensures that every those controller always be running. Contollers like replica set controller or auto healing controller, auto scaling controller.

>**Cloud controller manager [CCM]** Which translate user inputs or data to API request that cloud provider understand.
## Is kubernetes is tightly coupled or loosely coupled
> Kubernetes itself is designed to be a loosely coupled system — and that’s one of the main reasons it’s so powerful and flexible.
##### What Does “Loosely Coupled” Mean in Kubernetes?
> Each component in the Kubernetes architecture is designed to perform a specific task, communicate via well-defined APIs, and operate independently of others.
##### What About Pods?
> Inside a pod, the containers are:
> - Tightly coupled — because they share IP, volumes, lifecycle
> - But the Kubernetes components managing those pods? Loosely coupled.

## Difference between Stateless and statefull applications
#### Stateless Apps
> - Treat every request as independent
> - No knowledge of what happened before
> - Common in web servers, APIs, microservices
> *Use case:* Think of it like a vending machine — you press a button, get what you want, and the machine forgets about it.
#### Stateful Apps
> - Need to remember previous interactions
> - Often use local storage or in-memory data
> - Common in databases, queues, user sessions
> *Use case:* Like a bank account — the system remembers your balance across requests
#### Cloud & Kubernetes Perspective
> - **Stateless apps:** ideal for containers and auto-scaling
> - **Stateful apps:** need persistent volumes, stateful sets, and special care in scheduling

## What are the objects used in kuberenets for Stateful and statefull applications?
```
Application Type | Key Kubernetes Objects Used
Stateless        | Deployment, ReplicaSet, Pod, Service, ConfigMap, Secret, Ingress
Stateful         | StatefulSet, PersistentVolume (PV), PersistentVolumeClaim (PVC), Headless Service
```
#### Stateless Applications
```
Object     |          Purpose
Deployment | Manages replica pods for stateless apps (e.g., web server, API)
ReplicaSet | Ensures a specified number of pod replicas are running
Pod        | The smallest unit — can run one or more containers
Service    | Exposes pods to each other or to the internet (ClusterIP, NodePort, LB)
Ingress    | Routes external HTTP/S traffic to Services
ConfigMap  | Injects config data into containers as files or env variables
Secret     | Same as ConfigMap but used for sensitive info (passwords, tokens)
```
#### Stateful Applications
```
Object                | Purpose
StatefulSet           | Like a Deployment but for stateful apps — ensures stable IDs and storage
PersistentVolume      | Provides a piece of storage in the cluster (backed by disk, EBS, etc.)
PersistentVolumeClaim | A request for storage that pods can use
Headless Service      | Allows direct DNS to pods (e.g. mysql-0.mydb.default.svc.cluster.local)
Service               | Still used for exposure, sometimes combined with headless
```

## Example pod.yml
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

## How many containers run in a pod?
> In Kubernetes, a Pod can run:
> - One or more containers.
> - There’s no hard limit imposed by Kubernetes itself, but here's how it works:
##### Typical Scenarios
> - Most pods have 1 container — this is the standard pattern.
> - In advanced cases, you might have multiple containers in a Pod that:
>     - Share the same network namespace
>     - Can communicate over localhost
>     - Can share storage volumes
>     - These are called sidecar containers or init containers, often used for logging agents, proxies, or configuration.
> In practice, most clusters stay below 5–10 containers per pod unless you have a very specific use case (e.g., service meshes).

## Why we cannot run multiple containers in a single pod?
> We avoid running multiple containers in a Pod because it causes tight coupling, scaling problems, and management complexity. Pods should be simple and single-purpose.
OR
> Technically, you can run multiple containers in a Pod. BUT in practice, we avoid it unless necessary because of these reasons:
##### 1. Tight Coupling Problem
> - Containers in a Pod are tightly linked.
> - If one container crashes, it might affect others.
> - Harder to independently scale, update, or restart containers.
> **In Kubernetes, it's better to keep apps loosely coupled for flexibility.**
##### 2. Scaling Issues
> - Pods scale as a single unit.
> - You cannot scale one container inside a Pod independently.
> **Example: If your app container needs 5 replicas but your logging sidecar doesn't, you still have to scale both together.**
##### 3. Resource Sharing and Limits
> - Containers share CPU, memory, network, etc.
> - If one container is greedy (uses too much memory/CPU), it can starve the others inside the same Pod.
##### 4. Complexity in Management
> - Logs, monitoring, troubleshooting become more complicated when multiple containers are running together.
> - Harder to debug if the Pod crashes — which container caused it?
##### 5. Deployment and CI/CD Complexity
> - Separate containers mean separate CI/CD pipelines are easier.
> - Multi-container Pods mean you have to build/test/deploy everything together, which can slow you down.

## Services of kubernetes or Different types of services in Kubernetes

### ClusterIP (Default): 
>    - Exposes the Service to other Pods within the cluster.
>    - Accessible only within the cluster using the Service's cluster-internal IP or DNS name.
>    - Example Use Case: Backend services communicating with frontend Pods.
```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

### NodePort
>    - Exposes the Service on a static port on each node in the cluster.
>    - External clients can access the Service by reaching any node's IP address and the exposed port.
>    - Use Case: Basic external access for development or testing environments.
```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
      nodePort: 30007
```

### LoadBalancer
>    - Exposes the Service to the external world using a cloud provider's load balancer (e.g., AWS ELB, GCP Load Balancer, Azure LB).
>    - Automatically provisions an external IP address and routes traffic to the Service.
>    - Use Case: Exposing applications to the internet in a production environment.
```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

## How can we connect deployment.yml and service.yml
>The Deployment manages the Pods that run the application, while the Service exposes the Pods and provides a stable networking endpoint. Selectors and labels are used to connect both the files

### Selector and Labels:
>The Service uses the selector field to match the labels defined in the Deployment's Pod template.
>Example:
>Deployment Pod template labels: app: nginx
>Service selector: app: nginx
### Deplyment.yml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx    **This should match with the selector value in service.yml file**
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
```

### Service.yml
```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx        **This should match with the label template value in deployment.yml file**
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer
```

## What is the main differnece between etcd and persistent volumes
>- **etcd:** etcd is a distributed key-value store used by Kubernetes to store its cluster's state and configuration data.
>- Ex: When you create a Pod, its configuration is stored in etcd as part of the cluster's desired state.

>- **Persistent volumes:** PVs are a storage abstraction in Kubernetes that provide durable, long-term storage for applications running in Pods. Provide data storage for databases, logs, or any application that requires persistent data
>- Ex:  A MySQL database running in a Pod can use a Persistent Volume to store its database files, ensuring that data persists even if the Pod is deleted or rescheduled.

### Persistent volume - pvc.yml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: "/mnt/data"  # Path on the node
```

## What is ingress controller.
> An Ingress Controller is a specialized controller in Kubernetes that manages Ingress resources, allowing HTTP and HTTPS routing to services within the cluster. It acts as an entry point to the cluster.

> - **Ingress Resource**: A Kubernetes API object that defines rules for routing external HTTP/HTTPS traffic to cluster services.
> - **Ingress Controller**: Implements the rules defined in the Ingress resource. Popular controllers include NGINX Ingress Controller, Traefik, HAProxy, and others.

### ingress.yaml
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /frontend
            pathType: Prefix
            backend:
              service:
                name: frontend
                port:
                  number: 80
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api
                port:
                  number: 80
  tls:
    - hosts:
        - example.com
      secretName: example-tls
```
## Advantages of Ingress:
>- **Path-based Routing**: Route traffic based on URL paths (e.g., /api or /frontend).
>- **Weight or ratio based routing**: we can define ratio between the pods to distribute traffic
>- **Name-based Virtual Hosting**: Route traffic based on hostnames (e.g., example.com or api.example.com).
>- **SSL Termination**: Terminate HTTPS connections and forward plain HTTP to backend services.
>- **Load Balancing**: Distribute traffic across multiple replicas of services.
>- **Rewrite Rules**: Rewrite URLs for backend services.

## Why Do We Use Init Containers in Kubernetes?
> Init containers are special containers in a Kubernetes pod that run before the main application containers start. They're used for setup tasks that must be completed before the app starts.
#### Key Reasons We Use Init Containers
```
Purpose            | Explanation
Environment Setup  | Prepare configs, mount secrets, or set permissions
Dependency Wait    | Wait for services like DBs, APIs, or caches to be ready
Pre-processing     | Run DB migrations, download files, or setup shared volumes
Security Separation| Use different images or users with different privileges
Reusable Tasks     | Separate init logic from app logic — better modularity
```
#### How They Work
> - Init containers run in order, one at a time.
> - Each must succeed (exit 0) before the next starts.
> - Only after all init containers finish, the main containers start.

## Auto-scaling and types
> Auto-scaling in Kubernetes allows resources in the cluster to dynamically adjust based on workload demands. This ensures efficient resource utilization, cost-effectiveness, and high availability of applications.

### Horizontal Pod Autoscaler (HPA): POD Scaling
>**POD Scaling**: Automatically adjusts the number of Pod replicas for a deployment, replication controller, or replica set based on observed CPU/memory utilization or custom metrics. POD Scaling
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app  # Target Deployment to scale
  minReplicas: 2  # Minimum replicas
  maxReplicas: 10  # Maximum replicas
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # Scale when CPU usage exceeds 50%
```
### Vertical Autoscaler (VPA): CPU Scaling
>**CPU Scaling**: Automatically adjusts the resource requests and limits (CPU and memory) of Pods to ensure they have the appropriate resources.
```
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app  # Name of the deployment to scale
  updatePolicy:
    updateMode: "Auto"  # Options: Auto, Off, Initial
  resourcePolicy:
    containerPolicies:
    - containerName: nginx
      minAllowed:
        cpu: "100m"  # Minimum CPU
      maxAllowed:
        cpu: "1000m" # Maximum CPU
      controlledResources: ["cpu"]  # Only scale CPU
```

### Cluster Autoscaler: Node scaling
>Automatically adjusts the number of nodes in a Kubernetes cluster based on the requirements of scheduled Pods.

## What is auto healing
> **Auto-healing** in Kubernetes refers to the cluster's ability to automatically detect and recover from failures in its components, ensuring that the desired state of the system is always maintained.

>**Liveness and Readiness Probes**: Kubernetes uses liveness probes and readiness probes to check if a Pod is healthy. If the liveness probe fails (e.g., the application crashes), Kubernetes will automatically restart the Pod. If the readiness probe fails, Kubernetes will stop sending traffic to the Pod until it is healthy again.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 5
```

>**Pod Auto-Healing**: Kubernetes ensures that Pods are always running in the desired state. If a Pod fails or is terminated, Kubernetes will automatically attempt to restart or replace it.

>**ReplicaSet**: A ReplicaSet ensures that the specified number of replicas of a Pod are always running. If one of the Pods fails, the ReplicaSet creates a new Pod to replace it.
> - Example: If you define a ReplicaSet with 3 replicas and one Pod crashes, Kubernetes will automatically create a new Pod to ensure 3 Pods are always running.

>**Node Auto-Healing**: Kubernetes ensures that workloads continue running even when nodes fail. If a node becomes unresponsive or goes down, Kubernetes will detect this failure and reschedule the Pods that were running on that node to healthy nodes.

>**Pod Replacement Strategy**: The Pod replacement strategy defines how Pods are replaced during updates. Kubernetes supports several strategies for updating Pods in a Deployment, including Rolling Update, Recreate, and Blue-Green (though the latter typically requires external tools).

## When we will use maxsurge and maxunavailable in kubernetes?
> maxSurge and maxUnavailable are used in Kubernetes rolling updates to control how Pods are updated during a deployment, ensuring high availability and zero downtime during app updates.
##### When to use them?
> 1. **maxSurge**
> - Use when you want zero downtime or better availability.
> - Allows extra pods to be created before old ones are terminated.
> - E.g., maxSurge: 1 allows 1 extra pod → more availability during update.
> 2. **maxUnavailable**
> - Use to limit impact during update.
> - Controls how many pods can be down at any time.
> - E.g., maxUnavailable: 1 ensures at least replicas - 1 are always running.
>**Rolling Updates**: A rolling update is a deployment strategy where the old Pods are replaced by new ones in a controlled manner. Kubernetes handles this automatically when a Deployment is updated, ensuring that the application continues to run during the update.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3  # Ensures at least 3 Pods are running
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1  # Allows 1 extra Pod above desired replicas
      maxUnavailable: 1  # Only 1 Pod can be unavailable at a time
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: nginx
        image: nginx:1.20  # Initial version
        ports:
        - containerPort: 80
```

##### Best Practice
```
Goal             |      Setting
Zero downtime    | maxSurge: 1, maxUnavailable: 0
Faster updates   | maxSurge: 100%, maxUnavailable: 100% (caution!)
Safe + available | maxSurge: 1, maxUnavailable: 1 (balanced)
```

## Blue-Green Deployment (External Tool): 
>A Blue-Green Deployment is another popular update strategy but is not natively implemented in Kubernetes. It involves having two environments (Blue and Green) where:
>    - The Blue environment is the current live version.
>    - The Green environment is the new version.
>    - Once the Green environment is fully deployed and tested, traffic is switched from Blue to Green. Kubernetes can manage this strategy using tools like ArgoCD or Spinnaker.
#### Blue Deployment (Current version running)
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: blue
  template:
    metadata:
      labels:
        app: my-app
        version: blue
    spec:
      containers:
      - name: nginx
        image: nginx:1.20  # Blue version
        ports:
        - containerPort: 80
```
#### Green Deployment (New version to deploy)
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: green
  template:
    metadata:
      labels:
        app: my-app
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.21  # Green version
        ports:
        - containerPort: 80
```
#### A Kubernetes Service routes traffic to either blue or green version.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: green
  template:
    metadata:
      labels:
        app: my-app
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.21  # Green version
        ports:
        - containerPort: 80
```

## Canary deployment: 
>A Canary Deployment is a deployment strategy that allows you to release a new version of an application to a small subset of users (known as the "canary group") before rolling it out to the entire user base. The goal is to reduce risk by testing the new version with real users and production data in a controlled manner.

### How Canary Deployment Works
>- Initial Rollout: The new version of the application (the "canary" version) is deployed alongside the current stable version of the application.
>- Traffic Split: A portion of the traffic is routed to the canary version, while the majority continues to use the stable version.
>- Monitoring: The canary version is monitored for issues like performance degradation, errors, or crashes. If everything goes well, the rollout proceeds.
>- Gradual Increase: Gradually, more traffic is directed to the canary version.
>- Full Rollout: If the canary version is successful and no issues arise, the rollout continues to all users.

## How can we manage the traffic during the deployment in kubernetes?
### Using ingress controller
> Ingress controller like NGINX Ingressor Trafik can route traffic based on weights, path or headers.
>
> Steps:
>  - Deploy blue and green versions as separate deployments or set of pods
>  - Configure the ingress resource with:
>      - Weighted traffic routing[E.g: 90% Blue or 10% Green]
>      - Caanary releases by gradually increasing traffic to the green version
>  - Update the weight to 100% Green is ready.
OR 
> In a blue-green deployment in Kubernetes, traffic sharing isn't typically part of the classic model — you usually switch 100% of traffic from blue (old) to green (new). However, if you want to split traffic between blue and green (like 90/10, 50/50, etc.), you need to introduce traffic control via an advanced routing solution.
##### 1: Use Istio (or any Service Mesh)
> Istio allows fine-grained traffic routing between versions of a service.
> Example:
> 1. Deploy Blue and Green Versions
> Label your Deployments differently:
```
labels:
  app: myapp
  version: blue   # or green
```
> 2. Define a VirtualService in Istio
```
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
    - myapp.example.com
  http:
    - route:
        - destination:
            host: myapp
            subset: blue
          weight: 80
        - destination:
            host: myapp
            subset: green
          weight: 20
```
> 3. DestinationRule to Define Subsets
```
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
spec:
  host: myapp
  subsets:
    - name: blue
      labels:
        version: blue
    - name: green
      labels:
        version: green
```
##### 2: NGINX Ingress with Canaries (limited traffic split)
> If you're using the NGINX Ingress Controller, it supports canary releases.
```
# Base Ingress (blue)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-blue
                port:
                  number: 80
```
```
# Canary Ingress (green)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-canary
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "20"
spec:
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-green
                port:
                  number: 80
```
> This sends 20% of traffic to the green version.

## How Many Master Nodes & Worker Nodes?
> EKS
> - Master nodes = 3 (minimum, managed by AWS in multiple AZs — you don’t control them directly)
> - Worker nodes = Depends on your workload
> Example Prod Setup:
> - 3 Master Nodes (managed by AWS in 3 AZs)
> - 6–12 Worker Nodes (spread across 3 AZs using managed node groups)
> Example Dev/Test Setup:
> - 3 Master Nodes (default)
> - 2–4 Worker Nodes (small EC2 instances or Fargate)

## What tool have you used to communicate with Masternode and which CLI?
> **kubectl** — the official Kubernetes CLI tool
> It talks to the Kubernetes API server (which is part of the master node) to manage and interact with the cluster.
##### How It Works
> When you run something like:
```
kubectl get pods
```
> This happens under the hood:
> - kubectl reads your kubeconfig file (usually at ~/.kube/config)
> - It connects to the Kubernetes API server over HTTPS
> - Authenticates using token, certificate, or IAM (in EKS)
> - API server responds with requested data or takes action

## What is a StatefulSet in Kubernetes?
>- A StatefulSet in Kubernetes is a workload API object used to manage stateful applications. Unlike Deployments, which are designed for stateless applications, StatefulSets ensure that pods are created, updated, and deleted in a predictable and ordered manner. 
>- StatefulSets are particularly useful for applications that require stable network identities, persistent storage, and ordered scaling or deployment.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  minReadySeconds: 10
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: ebs
      resources:
        requests:
          storage: 1Gi
```

## Incase of stateful application which type of service used
> In stateful applications, the pods need stable network identities (e.g., stable DNS names) so they can maintain communication with each other in a consistent manner. A Headless Service provides this functionality by not assigning a cluster IP, which allows Kubernetes to expose the individual pod IPs instead.
> *Key Points:*
> - Headless Service doesn't have a clusterIP (or it is set to None), meaning Kubernetes doesn't assign an IP to the service.
> - It allows you to access the individual pods directly by their DNS names, which is essential for stateful applications like databases or caches where each pod has a unique role or stores different parts of the state.
#### Headless Service YAML:
```
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
spec:
  clusterIP: None  # Headless service, no ClusterIP assigned
  selector:
    app: mysql
  ports:
    - port: 3306
      targetPort: 3306
```
#### StatefulSet YAML (with PVC for persistent storage):
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: "mysql-headless"
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql:5.7
          volumeMounts:
            - name: mysql-pvc
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: mysql-pvc
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
```
> - The Headless Service enables stable DNS names for each pod: mysql-0.mysql-headless, mysql-1.mysql-headless, and so on.
> - This allows the pods to communicate with each other in a stateful manner and ensures each pod maintains its identity even after restarts.
#### Other Service Types for Stateful Applications:
> - While Headless Services are the most common and essential for StatefulSets, other service types like ClusterIP, NodePort, or LoadBalancer might also be used depending on the need to expose stateful applications externally.
> - ClusterIP can still be used for internal access when needed.
> - NodePort and LoadBalancer can be used to expose stateful applications to the outside world, but they are generally not required for inter-pod communication.

## It is a scenario based question in kubernetes "we had 3 pods pod1, pod2, pod3. Pod1 should not talk with pod2, pod2 can talk with pod3 and pod3 can talk with pod1 and pod2" how can we achieve this?
> To implement the given communication restrictions between the pods in Kubernetes, you can use Network Policies. These policies allow you to control the traffic flow between pods based on labels and namespaces.
>
> Scenario Recap:
> - Pod1 → Pod2: Not allowed
> - Pod2 → Pod3: Allowed
> - Pod3 → Pod1, Pod2: Allowed
##### Step 1: Label the Pods
> Make sure each pod is uniquely labeled. For example:
```
# For Pod1
metadata:
  name: pod1
  labels:
    app: pod1

# For Pod2
metadata:
  name: pod2
  labels:
    app: pod2

# For Pod3
metadata:
  name: pod3
  labels:
    app: pod3
```
##### Step 2: Define Network Policies
> 1. Deny Pod1 from accessing Pod2
> You actually don’t allow anything by default, then only allow specific traffic. So you can use a default deny for Pod2, and only allow Pod2 ← Pod3.
> 2. Create Default Deny for Pod2
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-pod2
spec:
  podSelector:
    matchLabels:
      app: pod2
  policyTypes:
    - Ingress
```
> This denies all incoming traffic to Pod2.
> 3. Allow Pod2 to receive from Pod3
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-pod3-to-pod2
spec:
  podSelector:
    matchLabels:
      app: pod2
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: pod3
```
> 4. Allow Pod1 to receive from Pod3
> If you want Pod3 to talk to Pod1 as well (since Pod1 should not talk to Pod2, but can receive from Pod3), you can define:
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-pod3-to-pod1
spec:
  podSelector:
    matchLabels:
      app: pod1
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: pod3
```
## User want some info it should connect to database and give output to the end user and should be secure. Loadbalancer should not connect with the application. How can we achieve this using K8s
> If the user-facing setup must connect securely to a database, return output, and should not be directly exposed to a LoadBalancer, you can achieve this using a secure internal architecture in Kubernetes. Here’s a solution that avoids exposing your app via a LoadBalancer and still lets users access it securely.
##### Solution Design (Secure Architecture)
> Components:
> - Ingress Controller (e.g., NGINX) – Acts as the secure entry point (instead of LoadBalancer).
> - Ingress Resource – Routes traffic securely (HTTP/S) to the internal app.
> - Backend Application (App Pod) – Connects to the DB and returns data.
> - Database Pod/Service – Stores data securely.
> - Secrets – Store DB credentials.
> - Network Policies (optional) – Lock down traffic to/from components.
##### Why No LoadBalancer?
> - Instead of exposing the app with Service type: LoadBalancer, expose only the Ingress Controller using a LoadBalancer.
> - All external traffic comes via Ingress → internal services, keeping app and DB secure.
##### How To Do It
> 1. Deploy Ingress Controller (like NGINX)
> Install it via Helm or manifest:
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/cloud/deploy.yaml
```
> 2. Create the Application Deployment and Service
```
# app-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: secure-app
  template:
    metadata:
      labels:
        app: secure-app
    spec:
      containers:
        - name: app
          image: your-app-image
          ports:
            - containerPort: 8080
          env:
            - name: DB_HOST
              value: "db-service"
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: db-secret
                  key: username
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secret
                  key: password
---
apiVersion: v1
kind: Service
metadata:
  name: app-service
spec:
  selector:
    app: secure-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP  # NOT LoadBalancer
```
> 3. Create an Ingress Resource
```
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: yourdomain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: app-service
                port:
                  number: 80
```
> 4. Database Deployment + Service + Secret
> Same as earlier: deploy DB with a ClusterIP service and load credentials using a Secret.

## What is a Helm Chart? (More detailed)
> A Helm chart is a package that defines, installs, and manages Kubernetes applications. It's not just templates — it's a structured folder with all the necessary Kubernetes YAMLs (Deployment, Service, Ingress, ConfigMap, etc.), parameterized using variables (values.yaml).
>
> Helm simplifies:
> - Templating: Replace hardcoded stuff like image tags, resource limits, secrets, etc., with variables.
> - Versioning: You can version your entire application setup (e.g., my-app-1.2.3).
> - Deployment: Install/upgrade/rollback a full app (with dependencies) in one command.
> - Configuration: Provide defaults via values.yaml, override them during helm install or helm upgrade.
```
mychart/
├── Chart.yaml       # Metadata about the chart
├── values.yaml      # Default config values
├── templates/       # Actual k8s YAML templates
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── ingress.yaml
│   └── _helpers.tpl # Reusable template snippets
└── charts/          # Dependency charts
```
> Use Cases of Helm Charts
> - Deploying microservices: E.g., a shopping app with separate charts for payment, catalog, checkout.
> - Platform tools setup: Installing Prometheus, Grafana, Istio — all available as charts.
> - Blue/Green or Canary deployments: With customized values.yaml controlling replicas, traffic splits.
> - Multi-environment config: Different values for dev, staging, and prod without changing code.
> - Reusable components: Build a chart once (say for a generic Node.js app) and reuse it across teams.

# Debugging:

### You node is perfoming slow and how will you troubleshoot in written commands
```
# Check node status
kubectl get nodes

# Describe node
kubectl describe node <node-name>

# Check resource usage
kubectl top node

# Check running pods and their usage
kubectl top pod --all-namespaces

# SSH into node and check system load
top, iotop, df -h, dmesg
```
OR
> Step-by-step approach with common Kubernetes commands and system diagnostics to help you identify performance issues.
##### 1. Check Node Resource Usage
>   - Start by checking the node's resource usage, such as CPU, memory, and disk. This helps determine if resource exhaustion is the cause of the slow performance.
>   - Check Node Resource Usage:
```
kubectl top nodes
```
>   - This will show the resource usage (CPU and Memory) of all nodes in the cluster.
>   - If you notice that the node is running out of resources, consider scaling the workload or optimizing resource requests and limits.
##### 2. Inspect Node Health and Status
> Check the status of the node. A node might be marked as NotReady due to various reasons (e.g., resource pressure, network issues).
> - Check Node Status:
```
kubectl get nodes
```
> - Check for the STATUS column. If it's not Ready, it indicates that the node has issues.
> To see detailed status:
```
kubectl describe node <node-name>
```
> In the description output, look for:
> - Conditions: Check if any condition is NotReady or has problems like DiskPressure, MemoryPressure, or NetworkUnavailable.
> - Allocatable Resources: Check how much CPU and memory are available versus what is being used.
##### 3. Analyze Pod Resource Usage on the Node
> Check if the pods running on the node are consuming too many resources.
> - Get Pods Running on the Node:
```
kubectl get pods --all-namespaces -o wide | grep <node-name>
```
> This will list all the pods running on the specific node.
> - Check Resource Usage of Pods:
```
kubectl top pod --all-namespaces --node <node-name>
```
> This command shows the CPU and memory usage of all pods on the node. Look for pods that are consuming excessive resources.
##### 4. Check Disk Usage
> If disk space is an issue, Kubernetes may throttle pods or fail to schedule new ones.
> - Check Node Disk Usage:
```
df -h
```
> This command will give you the disk space usage on the node. If the disk is full or close to full, that could cause node slowness.
##### 5. Check Network Latency or Issues
> Network performance issues can also contribute to node slowness, especially if nodes are not communicating properly.
> - Check Network Interfaces and Stats:
```
ifstat
```
> This command will show the network statistics for the node, such as bytes sent and received. It helps to identify any network bottlenecks.
> - Check Node Network Issues:
```
ping <node-ip>
```
> Ping the node itself to check for any local network latency issues
##### 6. Check Node’s CPU and Memory Usage (System Level)
> If the node’s system is under heavy load (outside Kubernetes), it might affect Kubernetes performance as well.
> - Check CPU Usage:
```
top -n 1 -b | grep Cpu
```
> This shows the CPU usage of the system. Check if any process is consuming too much CPU.
> - Check Memory Usage:
```
free -h
```
> Check the available memory. If your system is running out of memory, Kubernetes might start killing pods or evicting them.
##### 7. Check for Resource Requests and Limits
> Ensure that resource requests and limits for the pods are properly set, as over-provisioning or under-provisioning can cause slowness.
> Check Resource Requests and Limits:
```
kubectl describe pod <pod-name>
```
> Check the pod's resource requests and limits to make sure they are not too low or too high.
##### 8. Reboot the Node (If Required)
> In some cases, after identifying the root cause (e.g., resource exhaustion, network issues, etc.), a node may need to be rebooted to reset the system and clear any issues.
> - Reboot the Node:
```
sudo reboot
```
> Rebooting can clear temporary system issues, but make sure to address the underlying cause before doing so to avoid recurrence.

### Imagepullbackoff: 
> When a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff.

>    - The status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image for reasons such as
>
>-        a. Invalid image name or
>-        b. Pulling from a private registry without imagePullSecret.

>    - The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay. Kubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).


### CrashLoopBackOff: 
> When you see "CrashLoopBackOff," it means that kubelet is trying to run the container, but it keeps failing and crashing. After crashing, Kubernetes tries to restart the container automatically, but if the container keeps failing repeatedly, you end up in a loop of crashes and restarts, thus the term "CrashLoopBackOff."

>    - It causes due to the
>-    a. Misconfiguration
>-    b. Errors in the Liveness Probes
>-    c. The Memory Limits Are Too Low
>-    d. Wrong Command Line Arguments
>-    e. Bugs & Exceptions

### Pods-not-scheduled: 
> In Kubernetes, the scheduler is responsible for assigning pods to nodes in the cluster based on various criteria. Sometimes, you might encounter situations where pods are not being scheduled as expected. This can happen due to factors such as node constraints, pod requirements, or cluster configurations.

>    a. Node selector: Node Selector is a simple way to constrain pods to nodes with specific labels. It allows you to specify a set of key-value pairs that must match the node's labels for a pod to be scheduled on that node. Usage: Include a nodeSelector field in the pod's YAML definition to specify the required labels.
```    
    spec:
    containers:
    - name: my-app
    image: my-image
    nodeSelector:
    disktype: ssd
```
>    b. Node Affinity: Node Affinity is a more expressive way to specify rules about the placement of pods relative to nodes' labels. It allows you to specify rules that apply only if certain conditions are met. Usage: Define nodeAffinity rules in the pod's YAML definition, specifying required and preferred node selectors.
```
    spec:
    containers:
    - name: my-app
    image: my-image
    affinity:
    nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
            - key: disktype
            operator: In
            values:
            - ssd
```