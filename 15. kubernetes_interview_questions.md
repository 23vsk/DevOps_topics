## Few questions on kubernetes
```
https://chatgpt.com/share/67ebe53d-ddf4-8011-83d8-04e31d3f9ee4
```
```
https://chatgpt.com/share/67ece876-dca0-8011-9ff0-cd009fa41fe4
```
## Helm charts
```
https://chatgpt.com/share/67f38bae-a5e0-8011-9d96-31cafbc897f7
```

## What is kubernetes?
> Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.
>(often abbreviated as K8s - The abbreviation K8s comes from: Kubernetes has 10 characters between the first letter K and the last letter s.)

## Advantages of Kubernetes
> Cluster in nature or behaviour, Auto healing, Auto Scaling, provides multiple enterprise level support[Load balancer, secure networking]

## Architecture of kubernetes?
> Control Plane [Master]- API Sever, etcd, scheduler, controller manager, cloud controller manager
>
>Data Plane [Worker Node]- Kubelet, kube-proxy, container-runtime
>
>1. We user want to create a POD request goes through **Control plane**
>2. In Control plan, **API Server** validates the Pod manifest to ensure it follows Kubernetes schema and syntax. It writes the desired state into **etcd**, the distributed key-value store, which acts as the cluster's source of truth.
>3. The **Kubernetes Scheduler** determines the best node for the Pod. Once the Scheduler selects a node, it updates the Pod object in **etcd** with the node assignment.
>4. The **Kubelet** detects the new Pod assignment. It communicates with the 'container runtime' (e.g., containerd, CRI-O) to perform the following:
>    - Pull the container image from the container registry (e.g., Docker Hub or a private registry) if it is not already cached on the node.
>    - Create and start the container(s) as specified in the Pod manifest.
>    - Configure networking and storage for the Pod.
>5. **Kube-proxy** will assign the ip-address, networking.
>6. **Persistent Volume Attachments (if any):** If the Pod specifies volumes (e.g., for persistent storage), Kubernetes ensures the volumes are provisioned, attached, and mounted to the container.
>7. **Readiness and Liveness Probes (if configured):** Kubernetes monitors the health of the container(s) in the Pod using readiness and liveness probes.
>    - If a container fails a liveness probe, Kubernetes restarts it.
>    - If a container fails a readiness probe, it is removed from the Service endpoints, ensuring it doesn't receive traffic.

## Role of Components
>**Kubelet** component will responsible for maintaining or managing a POD. It always ensures POD is always running

>**Container runtime** is responsible for managing the lifecycle of containers on each node in the cluster. The container runtime ensures that containers are created, started, stopped, and deleted as specified by Kubernetes.

>**Kube-proxy** basically acts as networking and allocate ip address, default load balancing capabilities. Generally it uses IP tables with is present in Linux machines.

>**API Server** is a component basically exposes to the world. It will takes all the inputs from users and validate the syntax.

>**Scheduler** is basically responsible for schudeling POD or resorces on kubernetes. It was decided by API server and according to that scheduler acts on it.

>**etcd** is a key value store which stores all the data or configurations of kubernetes. Basically it is a back-up of kubernetes.

>**Controller manager** has some controller which ensures that every those controller always be running. Contollers like replica set controller or auto healing controller, auto scaling controller.

>**Cloud controller manager [CCM]** Which translate user inputs or data to API request that cloud provider understand.

>**Node**: In Kubernetes, a node is a worker machine in the cluster where workloads (i.e., Pods) are deployed and executed.

>**POD**: A Pod is the smallest and most basic deployable unit in Kubernetes. It represents a single instance of a running process in the cluster and serves as a wrapper for one or more containers.

>**Cluster**: A Kubernetes cluster is a collection of machines (nodes) that work together to run, manage, and scale containerized applications.

> **Container**is a lightweight, portable, and self-sufficient unit that packages an application and its dependencies, allowing it to run consistently across different environments

## Example pod.yml
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

## Services of kubernetes:

### ClusterIP (Default): 
>    - Exposes the Service to other Pods within the cluster.
>    - Accessible only within the cluster using the Service's cluster-internal IP or DNS name.
>    - Example Use Case: Backend services communicating with frontend Pods.
```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

### NodePort
>    - Exposes the Service on a static port on each node in the cluster.
>    - External clients can access the Service by reaching any node's IP address and the exposed port.
>    - Use Case: Basic external access for development or testing environments.
```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
      nodePort: 30007
```

### LoadBalancer
>    - Exposes the Service to the external world using a cloud provider's load balancer (e.g., AWS ELB, GCP Load Balancer, Azure LB).
>    - Automatically provisions an external IP address and routes traffic to the Service.
>    - Use Case: Exposing applications to the internet in a production environment.
```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

## How can we connect deployment.yml and service.yml
>The Deployment manages the Pods that run the application, while the Service exposes the Pods and provides a stable networking endpoint. Selectors and labels are used to connect both the files

## Selector and Labels:
>The Service uses the selector field to match the labels defined in the Deployment's Pod template.
>Example:
>Deployment Pod template labels: app: nginx
>Service selector: app: nginx
### Deplyment.yml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx    **This should match with the selector value in service.yml file**
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
```

### Service.yml
```
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx        **This should match with the label template value in deployment.yml file**
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer
```

## What is the main differnece between etcd and persistent volumes
>**etcd:** etcd is a distributed key-value store used by Kubernetes to store its cluster's state and configuration data.
>Ex: When you create a Pod, its configuration is stored in etcd as part of the cluster's desired state.

>**Persistent** volumes: PVs are a storage abstraction in Kubernetes that provide durable, long-term storage for applications running in Pods. Provide data storage for databases, logs, or any application that requires persistent data
>Ex:  A MySQL database running in a Pod can use a Persistent Volume to store its database files, ensuring that data persists even if the Pod is deleted or rescheduled.

### Persistent volume - pvc.yml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: "/mnt/data"  # Path on the node
```

## What is ingress controller.
> An Ingress Controller is a specialized controller in Kubernetes that manages Ingress resources, allowing HTTP and HTTPS routing to services within the cluster. It acts as an entry point to the cluster.

>**Ingress Resource**: A Kubernetes API object that defines rules for routing external HTTP/HTTPS traffic to cluster services.
>**Ingress Controller**: Implements the rules defined in the Ingress resource. Popular controllers include NGINX Ingress Controller, Traefik, HAProxy, and others.

### ingress.yaml
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /frontend
            pathType: Prefix
            backend:
              service:
                name: frontend
                port:
                  number: 80
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api
                port:
                  number: 80
  tls:
    - hosts:
        - example.com
      secretName: example-tls
```
## Advantages of Ingress:
>**Path-based Routing**: Route traffic based on URL paths (e.g., /api or /frontend).
>**Weight or ratio based routing**: we can define ratio between the pods to distribute traffic
>**Name-based Virtual Hosting**: Route traffic based on hostnames (e.g., example.com or api.example.com).
>**SSL Termination**: Terminate HTTPS connections and forward plain HTTP to backend services.
>**Load Balancing**: Distribute traffic across multiple replicas of services.
>**Rewrite Rules**: Rewrite URLs for backend services.

## Auto-scaling and types
> Auto-scaling in Kubernetes allows resources in the cluster to dynamically adjust based on workload demands. This ensures efficient resource utilization, cost-effectiveness, and high availability of applications.

### Horizontal Pod Autoscaler (HPA): POD Scaling
>**POD Scaling**: Automatically adjusts the number of Pod replicas for a deployment, replication controller, or replica set based on observed CPU/memory utilization or custom metrics. POD Scaling
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app  # Target Deployment to scale
  minReplicas: 2  # Minimum replicas
  maxReplicas: 10  # Maximum replicas
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # Scale when CPU usage exceeds 50%
```
### Vertical Autoscaler (VPA): CPU Scaling
>**CPU Scaling**: Automatically adjusts the resource requests and limits (CPU and memory) of Pods to ensure they have the appropriate resources.
```
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app  # Name of the deployment to scale
  updatePolicy:
    updateMode: "Auto"  # Options: Auto, Off, Initial
  resourcePolicy:
    containerPolicies:
    - containerName: nginx
      minAllowed:
        cpu: "100m"  # Minimum CPU
      maxAllowed:
        cpu: "1000m" # Maximum CPU
      controlledResources: ["cpu"]  # Only scale CPU
```

### Cluster Autoscaler: Node scaling
>Automatically adjusts the number of nodes in a Kubernetes cluster based on the requirements of scheduled Pods.

## What is auto healing
> **Auto-healing** in Kubernetes refers to the cluster's ability to automatically detect and recover from failures in its components, ensuring that the desired state of the system is always maintained.

>**Liveness and Readiness Probes**: Kubernetes uses liveness probes and readiness probes to check if a Pod is healthy. If the liveness probe fails (e.g., the application crashes), Kubernetes will automatically restart the Pod. If the readiness probe fails, Kubernetes will stop sending traffic to the Pod until it is healthy again.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 5
```

>**Pod Auto-Healing**: Kubernetes ensures that Pods are always running in the desired state. If a Pod fails or is terminated, Kubernetes will automatically attempt to restart or replace it.

>**ReplicaSet**: A ReplicaSet ensures that the specified number of replicas of a Pod are always running. If one of the Pods fails, the ReplicaSet creates a new Pod to replace it.
>Example: If you define a ReplicaSet with 3 replicas and one Pod crashes, Kubernetes will automatically create a new Pod to ensure 3 Pods are always running.

>**Node Auto-Healing**: Kubernetes ensures that workloads continue running even when nodes fail. If a node becomes unresponsive or goes down, Kubernetes will detect this failure and reschedule the Pods that were running on that node to healthy nodes.

>**Pod Replacement Strategy**: The Pod replacement strategy defines how Pods are replaced during updates. Kubernetes supports several strategies for updating Pods in a Deployment, including Rolling Update, Recreate, and Blue-Green (though the latter typically requires external tools).

>**Rolling Updates**: A rolling update is a deployment strategy where the old Pods are replaced by new ones in a controlled manner. Kubernetes handles this automatically when a Deployment is updated, ensuring that the application continues to run during the update.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3  # Ensures at least 3 Pods are running
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1  # Allows 1 extra Pod above desired replicas
      maxUnavailable: 1  # Only 1 Pod can be unavailable at a time
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: nginx
        image: nginx:1.20  # Initial version
        ports:
        - containerPort: 80
```

## Blue-Green Deployment (External Tool): 
>A Blue-Green Deployment is another popular update strategy but is not natively implemented in Kubernetes. It involves having two environments (Blue and Green) where:
>    - The Blue environment is the current live version.
>    - The Green environment is the new version.
>    - Once the Green environment is fully deployed and tested, traffic is switched from Blue to Green. Kubernetes can manage this strategy using tools like ArgoCD or Spinnaker.
## Blue Deployment (Current version running)
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: blue
  template:
    metadata:
      labels:
        app: my-app
        version: blue
    spec:
      containers:
      - name: nginx
        image: nginx:1.20  # Blue version
        ports:
        - containerPort: 80
```
## Green Deployment (New version to deploy)
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: green
  template:
    metadata:
      labels:
        app: my-app
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.21  # Green version
        ports:
        - containerPort: 80
```
## A Kubernetes Service routes traffic to either blue or green version.
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: green
  template:
    metadata:
      labels:
        app: my-app
        version: green
    spec:
      containers:
      - name: nginx
        image: nginx:1.21  # Green version
        ports:
        - containerPort: 80
```

## Canary deployment: 
>A Canary Deployment is a deployment strategy that allows you to release a new version of an application to a small subset of users (known as the "canary group") before rolling it out to the entire user base. The goal is to reduce risk by testing the new version with real users and production data in a controlled manner.

### How Canary Deployment Works
>Initial Rollout: The new version of the application (the "canary" version) is deployed alongside the current stable version of the application.
>Traffic Split: A portion of the traffic is routed to the canary version, while the majority continues to use the stable version.
>Monitoring: The canary version is monitored for issues like performance degradation, errors, or crashes. If everything goes well, the rollout proceeds.
>Gradual Increase: Gradually, more traffic is directed to the canary version.
>Full Rollout: If the canary version is successful and no issues arise, the rollout continues to all users.

## How can we manage the traffic during the deployment in kubernetes?
### Using ingress controller
>Ingress controller like NGINX Ingressor Trafik can route traffic based on weights, path or headers
>Steps:
>  - Deploy blue and green versions as separate deployments or set of pods
>  - Configure the ingress resource with:
>      - Weighted traffic routing[E.g: 90% Blue or 10% Green]
>      - Caanary releases by gradually increasing traffic to the green version
>  - Update the weight to 100% Green is ready.

## What is a StatefulSet in Kubernetes?
>A StatefulSet in Kubernetes is a workload API object used to manage stateful applications. Unlike Deployments, which are designed for stateless applications, StatefulSets ensure that pods are created, updated, and deleted in a predictable and ordered manner. 
>StatefulSets are particularly useful for applications that require stable network identities, persistent storage, and ordered scaling or deployment.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  minReadySeconds: 10
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: ebs
      resources:
        requests:
          storage: 1Gi
```
## Debugging:

### 1. Imagepullbackoff: 
>When a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff.

>    - The status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image for reasons such as
>
>        a. Invalid image name or
>        b. Pulling from a private registry without imagePullSecret.

>    - The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay. Kubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).


### 2. CrashLoopBackOff: 
>When you see "CrashLoopBackOff," it means that kubelet is trying to run the container, but it keeps failing and crashing. After crashing, Kubernetes tries to restart the container automatically, but if the container keeps failing repeatedly, you end up in a loop of crashes and restarts, thus the term "CrashLoopBackOff."

>    - It causes due to the
>    a. Misconfiguration
>    b. Errors in the Liveness Probes
>    c. The Memory Limits Are Too Low
>    d. Wrong Command Line Arguments
>    e. Bugs & Exceptions

### 3. Pods-not-scheduled: 
>In Kubernetes, the scheduler is responsible for assigning pods to nodes in the cluster based on various criteria. Sometimes, you might encounter situations where pods are not being scheduled as expected. This can happen due to factors such as node constraints, pod requirements, or cluster configurations.

>    a. Node selector: Node Selector is a simple way to constrain pods to nodes with specific labels. It allows you to specify a set of key-value pairs that must match the node's labels for a pod to be scheduled on that node. Usage: Include a nodeSelector field in the pod's YAML definition to specify the required labels.
```    
    spec:
    containers:
    - name: my-app
    image: my-image
    nodeSelector:
    disktype: ssd
```
>    b. Node Affinity: Node Affinity is a more expressive way to specify rules about the placement of pods relative to nodes' labels. It allows you to specify rules that apply only if certain conditions are met. Usage: Define nodeAffinity rules in the pod's YAML definition, specifying required and preferred node selectors.
```
    spec:
    containers:
    - name: my-app
    image: my-image
    affinity:
    nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
            - key: disktype
            operator: In
            values:
            - ssd
```